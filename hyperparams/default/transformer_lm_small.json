{
    "arch": ["transformer"],
    "decoder-attention-heads": [8],
    "encoder-attention-heads": [8],
     "encoder-embed-dim": [16],
    "encoder-ffn-embed-dim": [512],
    "encoder-layers": [1],
    "decoder-embed-dim": [16],
    "decoder-ffn-embed-dim": [512],
    "decoder-layers":  [1],
    "lr": [1e-3],
    "dropout": [0.5],
    "lr-scheduler": ["inverse_sqrt"],
    "warmup-init-lr": [1e-5],
    "min-lr": [1e-9],
    "warmup-updates" : [1000],
    "mdl-batches-per-epoch": [3000],
    "mdl-train-examples": [1],
    "optimizer": ["adam"],
    "seed": [1,2,3,4],
    "--cross-self-attention": true
}
